
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML> 
<HEAD>
<!-- "$Id: DECOMP_AQM.html 1 2017-06-10 18:05:20Z coats $" -->
<TITLE> 
     Coupling-Mode Decomposed-AQM Modeling
</TITLE>
</HEAD>
   
<BODY BGCOLOR="#FFFFFF" 
      TOPMARGIN="15" 
      MARGINHEIGHT="15" 
      LEFTMARGIN="15" 
      MARGINWIDTH="15">
<!--#include virtual="/INCLUDES/header.html" -->

<H1>
    Distributed Coupling-Mode Decomposed-AQM Modeling System
</H1>

<H2> <A NAME="contents">Contents</A> </H2>
    <UL>
        <LI> <A HREF="#summary">Summary</A>
        <LI> <A HREF="#decomp">Domain Decomposition</A>
        <LI> <A HREF="#setup">Setting up Coupled-Mode Cooperative Modeling</A>
        <LI> <A HREF="#aqmaster">Program <CODE>aqmaster</CODE></A>
        <LI> <A HREF="#metserver">Program <CODE>metserver</CODE></A>
        <LI> <A HREF="#cplmode">I/O&nbsp;API Coupling Mode</A>
    </UL>

<HR> <!----------------------------------------------------------------->

<H2>  <A NAME="summary">Summary</A> </H2>

    The Distributed Coupling-Mode Decomposed-AQM Modeling System
    uses a cooperating system of &quot;normal&quot; air quality
    models on rectangular subdomains, and programs
    <CODE><STRONG>aqmmaster</STRONG></CODE> and
    <CODE><STRONG>metserver</STRONG></CODE>.  This approach may
    be thought of as a special case of the nesting approach used
    by the real-time MAQSIP air quality model:  each nest grid has
    a single air quality model working on just that grid; these AQM's
    cooperate as a coupled parallel system, with the coarser grids
    providing  time dependent boundary conditions to the finer grids
    nested within them, and potentially aggregation data on the coverage
    within the coarser grids of concnetrations on the finer grid nested
    within them.
    <P>
    
    The modeling process begins by decomposing the domain into
    rectangular subdomains that overlap properly, and then putting the
    description of these subdomains into an I/O&nbsp;API-standard
    <CODE><A HREF="GRIDDESC.html">GRIDDESC</A></CODE> file. The
    <CODE>metserver</CODE> program reads the full-domain grid-geometry,
    emissions, and meteorology files, and from them constructs windowed
    grid-geometry, emissions, and meteorology files for each subdomain
    in the decomposition.  At every advection step, the
    <CODE>aqmmaster</CODE> program assembles full-domain concentrations
    from the outputs of the subdomain air quality models, and provides
    time stepped boundary coonditions back to them, as well as
    producing full-domain concentration outputs.   The whole system is
    tied together with the coupling-mode extensions for the Models-3
    I/O&nbsp;API to perform distributed parallel domain-decomposed air
    quality modeling across a set of machines.
    <P>

    Notice that all of the scheduling, coordination, assembly, and
    extraction activities are managed by the <CODE>aqmmaster</CODE>
    program, so that the subdomain air quality models are unmodified
    (except for linking with the coupling-mode version of the
    I/O&nbsp;API library and the <CODE>PVM</CODE> library, in addition
    to the usual <CODE>netCDF</CODE> library).  <EM>The source code of
    the AQM is unaffected by this cooperating-process parallelism.  No
    more work writing schedulers, boundary-extractors, etc., needs to
    be done by the modeler!</EM>
    <P>

    Both programs <CODE>aqmmaster</CODE> and <CODE>metserver</CODE> 
    are Fortran-90 dynamically-sized programs that adapt at run time to
    the sets of met and chemistry variables being modeled, and to the
    grids being run.  They are basically independent of AQM being run,
    as well, as long as the AQM uses the Models-3 I/O&nbsp;API for
    input and output, uses the basic Models-3 scheme for meteorology
    file types, and avoids deadlocks, and as long as the gridded met
    files have all the variables necessary for windowing to produce
    subdomain boundary met files.
    <P>

    <STRONG>Back to <EM><A HREF="#contents">Contents</A></EM></STRONG>
    <P>

<HR> <!----------------------------------------------------------------->

<H2>  <A NAME="decomp">Domain Decomposition</A> </H2>

    
    <P>

    <H3>Example of a Domain Decomposition</H3>

    In this example, we begin with a grid having 18 rows and 24 columns.
    We decompose this domain into three subdomains, as illustrated
    below; for subdomain modeling purposes, each subdomain will be 
    extended with a &quot;halo&quot; by one cell along each internal
    boundary.
    <OL>
        <LI> <STRONG>Subdomain 1</STRONG>:  rows 1-18, columns 1-8
        <LI> <STRONG>Subdomain 2</STRONG>:  rows 1-9,  columns 9-24
        <LI> <STRONG>Subdomain 3</STRONG>:  rows 10-18, columns 9-24
    </OL>
    <IMG SRC="decomp.gif" WIDTH="562" HEIGHT="497">
    <P>
    <EM>The reason for requiring this halo is
    the fact that in all existing CMAQ and MAQSIP implementations,
    there are errors in the implementation of thickened-boundary
    advection.  If these errors were corrected, then the halos (and the
    computational overhead that goes with them) would no longer be
    necessary.</EM>  
    <P>

    The portion of Subdomain&nbsp;1 that is actually used to generate
    concentration field output is as described above; however, in order
    to preserve the full order accuracy of the horizontal advection
    numerics, the air quality model for Subdomain&nbsp;1 actually
    models a region with a &quot;halo&quot; one column wider (and
    extending into Subdomains&nbsp;2 and 3).  The boundary for this
    subdomain then &quot;lives&quot; partly on column&nbsp;10 of the
    original full domain.  It is the responsibility of the
    <CODE>aqmmaster</CODE> program to gather rows 1-18, columns 1-8
    from Subdomain&nbsp;1 for the full-domain concentration output, and
    to provide column&nbsp;10 as part of the (time-dependent) boundary
    values for Subdomain&nbsp;1.
    <BR>
    <IMG SRC="subdomain.gif" WIDTH="582" HEIGHT="617">
    <P>

    Similarly, the air quality model for Subdomain&nbsp;2 actually
    models a 10 row by 17 column region with an L-shaped halo; the
    boundary for  Subdomain&nbsp;2 includes portions of column&nbsp;6
    and row&nbsp;11, as illustrated below.  Subdomain&nbsp;3 is left as
    an exercise for the reader :-)
    <BR>Subdomain&nbsp;2 
    <IMG SRC="subdomain2.gif" WIDTH="562" HEIGHT="637">

    <P>
    <STRONG>Back to <EM><A HREF="#contents">Contents</A></EM></STRONG>
    <P>

<HR> <!----------------------------------------------------------------->

<H2><A NAME="setup">Setting up Coupled-Mode Cooperative Modeling</A></H2>

    The for using the new Coupling Mode of the Models-3 I/O&nbsp;API
    was developed as part of the MCNC<A HREF =
    "http://www.baronams.com:/projects/ppar/">Practical Parallel
    Computing Strategies Project</A>, which was partially funded by the
    National Center for Environmental Research and Quality Assurance,
    Office of Research and Development, U.S. Environmental Protection
    Agency, under the Science To Achieve Results Grants Program in high
    performance computing and communications.  The basic idea was that
    by changing the low-level <EM>data storage layer</EM> in the
    I/O&nbsp;API so that it had an alternative <EM>communications
    implementation</EM> in addition to the existing (netCDF-based)
    file-storage implementation, <STRONG>one could use existing
    single-topic models to build more complex cooperating process
    multi-topic coupled modeling systems,</STRONG>  the choice of
    storage mode being made at program launch on the basis of
    environment variables. Moreover, the individual single-topic models
    would not &quot;know&quot; (nor would they need to know!) at the
    source code level whether they were running stand-alone or as part
    of a coupled modeling system.  The only requirement for such
    coupled modeling was that input operations (I/O API
    <A HREF="OPEN3.html">OPEN3()</A> for input files,
    <A HREF="READ3.html">READ3()</A>, 
    <A HREF="INTERP3.html">INTERP3()</A>,
    <A HREF="XTRACT3.html">XTRACT3()</A>, and  
    <A HREF="DDTVAR3.html">DDTVAR3()</A>) should <EM>block</EM> when
    they request data that is not yet available (i.e., they should put
    the requester to sleep until the data's producer writes it out, and
    then wake up the requester and allow it to continue).  This is made
    possible by the selective direct access nature of I/O&nbsp;API
    calls, and in fact was one of the original design goals of the
    Models-3 system.
    <P>
 
    For the particular case of domain decomposed air quality modeling,
    the way this works is as follows:
    <UL>
        <LI>  Decompose your domain into rectangular subdomains, as
              <A HREF="#decomp">described above</A>.  (In the absence of
              other GUI tools for this purpose, the &quot;zoom&quot;
              feature of PAVE (acting. e.g., on a cross-point gridded
              met file) can be useful to help visualize subdomains
              as you do this task.
              <P>
        <LI>  Build executables for all the subdomain AQMs (this is
              necessary if the subdomain description &quot;compiles
              into&quot; the AQM executables, as it does with MAQSIP
              and with the current version of CMAQ; it is rumored not
              to be true for the <EM>next</EM> version of CMAQ).
              <P>
        <LI>  If running from file-based meteorology and/or emissions,
              run <CODE>metserver</CODE> in the appropriate mode to
              <P>
        <LI>  Compute the <CODE>IOAPI_KEEP_NSTEPS</CODE> necessary
              for each coupled-mode process.  This is the number of
              time steps that it is necessary to keep in PVM-mailbox
              memory for the system to operate. 
              <P>
        <LI>  Start up a PVM session that includes all the host machines
              being used to run the coupled modeling system.
              <P>
        <LI>  For each coupled-mode process, launch it in the
              background on the appropriate host machine.
              <P>
        <LI>  Wait until everything is completed.
              <P>
        <LI>  Shut down the PVM session.
              <P>
    </UL>
    <P>
    <STRONG>Back to <EM><A HREF="#contents">Contents</A></EM></STRONG>
    <P>

<HR> <!----------------------------------------------------------------->

<H2> <A NAME="aqmaster">Program <CODE>aqmaster</CODE></A> </H2>

    <H3>Description</H3>
    
    The sequence of operation for <CODE>aqmaster</CODE> is as follows:
    <OL>
        <LI>  read in all the control parameters (starting date,
              starting time, etc., as given in the <EM>Environment
              Variables</EM> section below.
        <LI>  Open the full-domain chemical initial and boundary
              condition files for input.  Note that this determines
              both the full-domain grid structure and the set of
              chemical species that will be modeled.
        <LI>  Create/opens all the subdomain chemical initial and
              boundary condition file for output.
        <LI>  Opens the subdomain chemical concentration files
              for input.
        <LI>  For every subdomain and for every chemical species:
              <OL>
                  <LI>  Window the full domain concentrations to the
                        subdomain grid.
                  <LI>  Write the subdomain concentrations grid to the
                        subdomain chemical initial condition file
              </OL>
        <LI>  For every time step:
             <OL>
                  <LI>  For every subdomain and for every chemical species:
                        <OL>
                            <LI>  Read in the subdomain concentrations
                                  grid from the subdomain concentration
                                  file.
                            <LI>  Aggregate the subdomain concentrations
                                  into the full domain concentrations
                                  grid.
                            <LI>  Construct the subdomain boundary
                                  concentrations.
                            <LI>  Write the subdomain boundary
                                  concentrations to the subdomain
                                  chemical boundary  condition file.
                        </OL>
              </OL>
    </OL>
    Notice that the order of operations is carefully laid out so as to
    avoid deadlocks, and so as to allow <CODE>aqmaster</CODE> to act
    as a component in a cooperating-process implementation of a nested
    AQM. It is also laid out so as to allow coupled
    <CODE>aqmaster</CODE> and <CODE>metserver</CODE> to operate within
    a cooperating process real time environmental modeling system with
    additional meteorological and emissions model components (avoiding
    race conditions in such a system, particularly, is the role of the
    optional <CODE>SYNCH_FILE</CODE>).
    <P>


    <H3>Required Environment Variables</H3>
    
    The execution of program <CODE>aqmaster</CODE> is controlled purely
    by environment variables, for easy scriptability.  Some of these
    variables are control parameter variables, others are <A
    HREF="LOGICALS.html">logical name</A> environment variables for the
    input and output files, which contain the path-names for the
    respective files, according to Models-3 conventions.  These
    environment variables may be set by the <VAR>csh
    <CODE>setenv</CODE></VAR> command, or by the  <VAR>sh</VAR> or
    <VAR>ksh <CODE>set</CODE></VAR> and <VAR><CODE>export</CODE></VAR> or
    <VAR><CODE>env</CODE></VAR> commands.  The list of environment
    variables for <CODE>aqmaster</CODE> is the following.
    <UL>
    <LI> <STRONG>Control Parameters</STRONG>
    <DL>
        <DT> <STRONG><CODE>STDATE</CODE></STRONG>
        <DD> starting date, given in <CODE>YYYDDD</CODE> according to 
             Models-3 conventions.
        <DT> <STRONG><CODE>STTIME</CODE></STRONG>
        <DD> starting time, given in <CODE>HHMMSS</CODE> according to 
             Models-3 conventions.
        <DT> <STRONG><CODE>CPLSTEP</CODE></STRONG>
        <DD> coupling time-step, <CODE>HHMMSS</CODE> (should match
             the advection  or model time step of a full domain AQM)
        <DT> <STRONG><CODE>OUTSTEP</CODE></STRONG>
        <DD> full domain output time-step, <CODE>HHMMSS</CODE>
        <DT> <STRONG><CODE>RUNLEN</CODE></STRONG>
        <DD> run duration,  <CODE>HHMMSS</CODE>
        <DT> <STRONG><CODE>GRIDDESC</CODE></STRONG>
        <DD> path name for the <A HREF="GRIDDESC.html">GRIDDESC file</A>
             used in Models-3 compliant systems to store grid and
             coordinate system descriptions.
        <DT> <STRONG><CODE>WINDOW_XGRIDS</CODE></STRONG>
        <DD> comma-delimited list of subdomain grid-names, as they
             appear in the <CODE>GRIDDESC</CODE> file.  The number of
             entries in this list determines the number
             <CODE>SUB_COUNT</CODE>of subdomains being modeled, and hence
             the numbers of subdimain initial condition, boundary
             condition, and concentration files used in the coupled
             modeling system.
        <DT> <STRONG><CODE>IOAPI_KEEP_NSTEPS</CODE></STRONG>
        <DD> number of time steps to keep in PVM mailbox buffers,
             computed in terms of the interaction between the
             emissions, meteorology, and air quality tiem steps of the
             models used in the coupled system.
    </DL>
             <P>
    <LI> <STRONG>Input File Logical Names</STRONG>
    <DL>
        <DT> <STRONG><CODE>CHEM_BDY_3D</CODE></STRONG>
        <DD> Logical name for the input full-domain chemical boundary
             condition file
        <DT> <STRONG><CODE>CHEM_INIT_3D</CODE></STRONG>
        <DD> Logical name for the input full-domain chemical initial
             condition file
        <DT> <STRONG><CODE>CHEM_CONC_3D</CODE></STRONG>
        <DD> Logical name for the output full-domain chemical 
             concentration file
    </DL>
             <P>
    <LI> <STRONG>Output Subdomain-File Logical Names</STRONG>
    <DL>
        <DT> <STRONG><CODE>CHEM_BDY_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical names for the output subdomain chemical boundary
             condition files
        <DT> <STRONG><CODE>CHEM_INIT_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical names for the output subdomain chemical initial
             condition files
         <DT> <STRONG><CODE>CHEM_CONC_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical names for the input subdomain chemical
             concentration files
    </DL>
             <P>
    <LI> <STRONG>Optional QA Output File Logical Names</STRONG>
    <DL>
        <DT> <STRONG><CODE>QA_CRO_2D</CODE></STRONG>
        <DD> Logical name for the optional domain decomposition QA-check
             file, or <CODE>NONE</CODE> to turn this file off.
        <DT> <STRONG><CODE>SYNCH_FILE</CODE></STRONG>
        <DD> Logical name for the optional synch-file to avoid race
             conditions with other models, or <CODE>NONE</CODE> to
             turn this file off.
    </DL>
    </UL>
    <P>

    <P>
    <STRONG>Back to <EM><A HREF="#contents">Contents</A></EM></STRONG>
    <P>

<HR> <!----------------------------------------------------------------->

<H2> <A NAME="metserver">Program <CODE>metserver</CODE></A> </H2>


    <H3>Description</H3>
    
    The sequence of operation for <CODE>metserver</CODE> is as follows:
    <OL>
        <LI>  read in all the control parameters (starting date,
              starting time, etc.), as given in the <EM>Environment
              Variables</EM> section below.  Options include turning
              on or off each of the families of files below; note that
              the sets of variables in each input file determine at
              runtime the sets of variables in the corresponding output
              subdomain files:
              <OL>
                  <LI>  <CODE>CHEM_EMIS_3D</CODE>
                  <LI>  <CODE>GRID_BDY_2D</CODE>
                  <LI>  <CODE>GRID_BDY_3D</CODE>
                  <LI>  <CODE>GRID_CRO_2D</CODE>
                  <LI>  <CODE>GRID_CRO_3D</CODE>
                  <LI>  <CODE>GRID_DOT_2D</CODE>
                  <LI>  <CODE>MET_BDY_2D</CODE>
                  <LI>  <CODE>MET_BDY_3D</CODE>
                  <LI>  <CODE>MET_CRO_2D</CODE>
                  <LI>  <CODE>MET_CRO_3D</CODE>
                  <LI>  <CODE>MET_DOT_2D</CODE>
                  <LI>  <CODE>MET_KF_2D</CODE>
                  <LI>  <CODE>MET_KF_3D</CODE>
              </OL>

        <LI>  Perform consistency checks:
              <OL>
                  <LI>  Subdomain grids fit together correctly to form
                        the full domain grid.
                  <LI>  If both are being produced, the time step for
                        the <CODE>CHEM_EMIS_3D</CODE> file must be an
                        exact multiple of, ro exactly the same as, the
                        met time step. If the met files are not being
                        produced set the met time step artificially to
                        be the emissions time step, to allow the
                        deadlock-free interleaved processing algorithm,
                        below.                        
                  <LI>  If <CODE>GRID_BDY_2D</CODE> is turned on,
                        then <CODE>GRID_CRO_2D</CODE> must be available
                        and must contain the needed variables;
                  <LI>  If <CODE>GRID_BDY_3D</CODE> is turned on,
                        then <CODE>GRID_CRO_3D</CODE> must be available
                        and must contain the needed variables;
                  <LI>  If <CODE>MET_BDY_2D</CODE> is turned on,
                        then <CODE>MET_CRO_2D</CODE> must be available
                        and must contain the needed variables;
                  <LI>  If <CODE>MET_BDY_3D</CODE> is turned on,
                        then <CODE>MET_CRO_3D</CODE> must be available
                        and must contain the needed variables;
              </OL>
        <LI>  If the <CODE>GRID_*_2D</CODE> files are being produced,
              then for each variable within them:
              <OL>
                  <LI>  Read the grid and boundary values of the
                        variable from the input files, as appropriate
                        (from both, if it is a boundary variable; from
                        the gridded file only, if it is a gridded-only
                        variable);
                  <LI>  If the variable is a boundary variable, construct
                        an &quot;expanded domain&quot; grid of that
                        variable (including both the boundary and the
                        cross-point-grid cells).
                  <LI>  For each subdomain:
                        <OL>
                            <LI>  If the variable is a boundary variable,
                                  extract/construct the subdomain
                                  boundary values from the
                                  &quot;expanded domain&quot; grid and
                                  write then to the subdomain boundary
                                  file.
                            <LI>  Extract the subdomain cross point
                                  gridded values from either the full
                                  domain or the &quot;expanded domain&quot;
                                  grid (as appropriate), and write then
                                  to the subdomain cross-point gridded
                                  file.
                        </OL>
              </OL>
        <LI>  Similarly for the <CODE>GRID_*_3D</CODE> files.
        <LI>  For each output met-file time step:
              <OL>
                  <LI>  If the <CODE>MET_*_2D</CODE> files are being
                        produced, then for each
                        variable within them:
                        <OL>
                            <LI>  Read the grid and boundary values of
                                  the variable from the input files, as
                                  appropriate (from both, if it is a
                                  boundary variable; from the gridded
                                  file only, if it is a gridded-only
                                  variable);
                            <LI>  If the variable is a boundary variable, 
                                  construct an &quot;expanded
                                  domain&quot; grid of that variable
                                  (including both the boundary and the
                                  cross-point-grid cells).
                            <LI>  For each subdomain:
                                  <OL>
                                      <LI>  If the variable is a boundary
                                            variable, extract/construct
                                            the subdomain boundary
                                            values from the
                                            &quot;expanded domain&quot;
                                            grid and write then to the
                                            subdomain boundary file.
                                      <LI>  Extract the subdomain cross point
                                            gridded values from either
                                            the full domain or the
                                            &quot;expanded domain&quot;
                                            grid (as appropriate), and
                                            write then to the subdomain
                                            cross-point gridded file.
                                  </OL>
                        </OL>

                        <P> <EM><STRONG>Note about KF Files:</STRONG>
                              the <CODE> MET_KF_*</CODE> are always
                              physical files (not virtual) and are
                              written in MM5 *before* the first
                              write to any <CODE>MET_CRO*</CODE> file;
                              in the AQM, they are read after several
                              reads from the <CODE>MET_CRO*</CODE>
                              files.  Sandwiching <CODE> MET_KF_*</CODE> 
                              processing between <CODE>MET_CRO_2D</CODE> 
                              and <CODE>MET_CRO_3D</CODE> processing
                              guarantees synchronization in coupling
                              mode operation.   Note also that for the
                              first time step iteration, we must be 
                              careful to &quot;capture&quot; all events
                              currently in progress.
                             </EM>

                        <LI> If the <CODE>MET_KF_2D</CODE> file
                             is being produced, then window it 
                             and write the result to the subdomain
                             files.

                        <LI> Similarly for the <CODE>MET_KF_3D</CODE>
                             files.

                        <LI> Process the <CODE>MET_CRO*_3D</CODE> files
                             in the same fashion as the
                             <CODE>MET_CRO_2D</CODE> files.

                        <LI> Similarly for the <CODE>MET_DOT_3D</CODE>
                             files.

                        <LI>  If the <CODE>CHEM_EMIS_3D</CODE> file
                              is being produced, for each emissions
                              variable:
                              <OL>
                                  <LI>  Read the grid and boundary values
                                        of the variable from the input
                                        file.
                                  <LI>  Extract the subdomain cross point
                                        gridded values from the full
                                        domain grid, and write then to
                                        the subdomain cross-point
                                        gridded file.
                              </OL>
              </OL>
    </OL>

    Notice that the order of operations is carefully interleaved so as
    to avoid deadlocks in a cooperating process environmental modeling
    system, and so as to allow <CODE>metserver</CODE> to act as a
    component in a cooperating-process implementation that includes
    concurrent meteorological and emissions models that generate the
    full-domain inputs to the distributed air quality model.
    <P>


    <H3>Required Environment Variables</H3>
    <BLOCKQUOTE>
    Execution of <CODE>metserver</CODE> is completely controlled by
    environment variables, for easy scriptability.  These may be set
    by the <VAR>csh <CODE>setenv</CODE></VAR> command, or by the 
    <VAR>sh</VAR> or <VAR>ksh <CODE>set</CODE></VAR> and
    <VAR><CODE>export</CODE></VAR> or <VAR><CODE>env</CODE></VAR>
    commands.  The list of environment variables for
    <CODE>metserver</CODE> is the following.
    <UL>
    <LI> <STRONG>Control Parameters</STRONG>
    <DL>
        <DT> <STRONG><CODE>STDATE</CODE></STRONG>
        <DD> starting date, given in <CODE>YYYDDD</CODE> according to 
             Models-3 conventions.
        <DT> <STRONG><CODE>STTIME</CODE></STRONG>
        <DD> starting time, given in <CODE>HHMMSS</CODE> according to 
             Models-3 conventions.
        <DT> <STRONG><CODE>TSTEP</CODE></STRONG>
        <DD> Meteorology time-step, <CODE>HHMMSS</CODE>
        <DT> <STRONG><CODE>ESTEP</CODE></STRONG>
        <DD> Emissions time-step, <CODE>HHMMSS</CODE>; must be a
             multiple of the meteorology time-step.
        <DT> <STRONG><CODE>RUNLEN</CODE></STRONG>
        <DD> run duration,  <CODE>HHMMSS</CODE>
        <DT> <STRONG><CODE>GRIDDESC</CODE></STRONG>
        <DD> path name for GRIDDESC file
        <DT> <STRONG><CODE>WINDOW_XGRIDS</CODE></STRONG>
        <DD> comma-delimited list of subdomain cross-point grid-names,
             as they appear in the <CODE>GRIDDESC</CODE> file.  The
             number of entries in this list determines the number
             <CODE>SUB_COUNT</CODE>of subdomains being modeled, and
             hence the numbers of subdimain meteorology and emissions
             files used in the coupled modeling system.
        <DT> <STRONG><CODE>WINDOW_DGRIDS</CODE></STRONG>
        <DD> comma-delimited list of subdomain dot-point grid-names.
             The number of entries in this list must match the number
             of entries in <CODE>WINDOW_XGRIDS</CODE>.
        <DT> <STRONG><CODE>IOAPI_KEEP_NSTEPS</CODE></STRONG>
        <DD> number of time steps to keep in PVM mailbox buffers,
             computed in terms of the interaction between the
             emissions, meteorology, and air quality time steps of the
             models used in the coupled system.
    </DL>
             <P>
    <LI> <STRONG>Input File Logical Names</STRONG>
    <DL>
        <DT> <STRONG><CODE>CHEM_EMIS_3D</CODE></STRONG>
        <DD> Logical name of the input full domain cross point
             3-D (layered) chemical emissions file, or 
             <CODE>NONE</CODE> to turn emissions processing off.       
        <DT> <STRONG><CODE>GRID_BDY_2D</CODE></STRONG>
        <DD> Logical name of the input full domain boundary point
             2-D grid geometry file, or  <CODE>NONE</CODE> to turn this
             part of the processing off.
        <DT> <STRONG><CODE>GRID_BDY_3D</CODE></STRONG>
        <DD> Logical name of the input full domain boundary point
             3-D (layered) grid geometry file, or  <CODE>NONE</CODE> to
             turn this part of the processing off.
        <DT> <STRONG><CODE>GRID_CRO_2D</CODE></STRONG>
        <DD> Logical name of the input full domain cross point gridded
             2-D grid geometry file, or  <CODE>NONE</CODE> to turn this
             part of the processing off.
             <BR>
             <EM>NOTE:  this file is required for
             <CODE>GRID_BDY_2D</CODE> processing.</EM>
        <DT> <STRONG><CODE>GRID_CRO_3D</CODE></STRONG>
        <DD> Logical name of the input full domain cross point gridded
             3-D (layered) grid geometry file, or  <CODE>NONE</CODE> to
             turn this part of the processing off.
             <BR>
             <EM>NOTE:  this file is required for
             <CODE>GRID_BDY_3D</CODE> processing.</EM>
        <DT> <STRONG><CODE>GRID_DOT_2D</CODE></STRONG>
        <DD> Logical name of the input full domain dot point gridded
             2-D grid geometry file, or  <CODE>NONE</CODE> to turn this
             part of the processing off.
        <DT> <STRONG><CODE>MET_BDY_2D</CODE></STRONG>
        <DD> Logical name of the input full domain boundary point 2-D
             meteorology file, or  <CODE>NONE</CODE> to turn this part
             of the processing off.
        <DT> <STRONG><CODE>MET_BDY_3D</CODE></STRONG>
        <DD> Logical name of the input full domain  boundary point
             3-D (layered) meteorology file, or  <CODE>NONE</CODE> to
             turn this part of the processing off.
        <DT> <STRONG><CODE>MET_CRO_2D</CODE></STRONG>
        <DD> Logical name of the input full domain cross point gridded
             2-D meteorology file, or  <CODE>NONE</CODE> to turn this
             part of the processing off.
             <BR>
             <EM>NOTE:  this file is required for
             <CODE>MET_BDY_2D</CODE> processing.</EM>
        <DT> <STRONG><CODE>MET_CRO_3D</CODE></STRONG>
        <DD> Logical name of the input full domain cross point gridded
             3-D (layered) meteorology file, or  <CODE>NONE</CODE> to
             turn this part of the processing off.
             <BR>
             <EM>NOTE:  this file is required for
             <CODE>MET_BDY_3D</CODE> processing.</EM>
        <DT> <STRONG><CODE>MET_DOT_3D</CODE></STRONG>
        <DD> Logical name of the input full domain dot point gridded
             3-D(layered) meteorology file, or  <CODE>NONE</CODE> to
             turn this part of the processing off.
    </DL>
             <P>
    <LI> <STRONG>Output Subdomain-File Logical Names</STRONG>
    <DL>
        <DT> <STRONG><CODE>CHEM_EMIS_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output cross point 3-D (layered)
             chemical emissions file, if this part of the processing
             is turned on.       
        <DT> <STRONG><CODE>GRID_BDY_2D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output boundary point 2-D grid 
             geometry file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>GRID_BDY_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output boundary point 3-D
             grid geometry file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>GRID_CRO_2D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output cross point gridded 2-D grid 
             geometry file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>GRID_CRO_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output cross point gridded 3-D
             (layered) grid geometry file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>GRID_DOT_2D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output dot point gridded 2-D grid 
             geometry file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>MET_BDY_2D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output boundary point 2-D
             meteorology file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>MET_BDY_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output  boundary point 3-D (layered)
             meteorology file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>MET_CRO_2D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output cross point gridded 2-D
             meteorology file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>MET_CRO_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output cross point gridded 3-D
             (layered) meteorology file, if this part of the processing
             is turned on.
        <DT> <STRONG><CODE>MET_DOT_3D_G&lt;nn&gt;</CODE></STRONG>,
             for <CODE>nn=1,...,SUB_COUNT</CODE>
        <DD> Logical name of the output dot point gridded 3-D
             (layered) meteorology file, if this part of the processing
             is turned on.
             <P>
    </DL>
    </UL>
    </BLOCKQUOTE>

    <P>
    <STRONG>Back to <EM><A HREF="#contents">Contents</A></EM></STRONG>
    <P>

<HR> <!----------------------------------------------------------------->

<H2> <A NAME="cplmode">I/O&nbsp;API Coupling Mode</A> </H2>

    As part of the 
    <A HREF = "http://www.baronams.com:/projects/ppar/index.html">Practical
    Parallel Project</A>, MCNC has developed an extended
    <STRONG>Model Coupling Mode</STRONG> for the I/O&nbsp;API.  
    This mode, implemented using 
    <A HREF = "http://www.epm.ornl.gov:80/pvm/">PVM&nbsp;3.4</A> 
    mailboxes, allows the user to specify in
    the run-script whether &quot;file&quot; means a physical file on
    disk or a PVM mailbox-based communications channel (a
    <STRONG>virtual file</STRONG>), on the basis of the value of the
    file's <A HREF = "ENVIRONMENT.html">logical name</A>:  
    <PRE><STRONG>
    setenv FOO                &quot;virtual BAR&quot;
    setenv IOAPI_KEEP_NSTEPS  3
    </STRONG></PRE>
    declares that <CODE>FOO</CODE> is the logical name of a virtual 
    file whose physical name (in terms of PVM mailbox names) is
    <CODE>BAR</CODE>.  The additional environment variable
    <CODE>IOAPI_KEEP_NSTEPS</CODE> determines the number of time 
    steps to keep in PVM mailbox buffers -- if it is 3 (as here), and 
    there are already 3 timesteps of variable <CODE>QUX</CODE> in the 
    mailboxes for virtual file <CODE>FOO</CODE>, then writing a fourth 
    time step of <CODE>QUX</CODE> to <CODE>FOO</CODE> causes the earliest 
    time step of <CODE>QUX</CODE> to be erased, leaving only timesteps 2, 
    3, and&nbsp;4.  This is necessary, so that the coupled modeling system
    does not require an infinite amount of memory for its sustained
    operation.  If not set, <CODE>IOAPI_KEEP_NSTEPS</CODE> defaults 
    to&nbsp;2 (the minimum needed to support <CODE>INTERP3()</CODE>'s
    double-buffering).
    <P>
    
    The (UNIX) environments in which the modeler launches multiple
    models  each of which reads or writes from a virtual file must all
    agree  on its physical name (usually achieved by sourcing some
    <VAR>csh</VAR> script that  contains the relevant
    <VAR><CODE>setenv</CODE></VAR> commands).
    <P>
    
    For models exchanging data via virtual files of the I/O&nbsp;API's
    coupling mode, the I/O&nbsp;API schedules the various processes on 
    the basis of data availability:
    <UL>
        <LI>  The modeler must start up a PVM session that will
              &quot;contain&quot; all the virtual files and enroll in
              it all those machines which will be running the various 
              modeling programs before starting up the various models
              in a coupled modeling system on those respective machines.
              <P>
        <LI>  <CODE>OPEN3()</CODE> calls for read-access to virtual
              files that haven't yet been opened for write access by
              some other process put the caller to sleep until the 
              file is opened; and
              <P>
         <LI>  <CODE>READ3()</CODE>, <CODE>INTERP3()</CODE>, or
              <CODE>DDTVAR3()</CODE> calls for virtual-file data which 
              has not yet been written put the reading process to sleep 
              until the data arrives, at which point the reader is 
              awakened and given the data it requested.  
    </UL>
    There are three requirements on the modeler: 
    <UL>
        <LI>  structuring reads and writes so as to avoid deadlocks 
              (two or more models, each asleep while waiting for input 
              from the other); and 
              <P>
        <LI>  providing enough feedbacks to prevent one process from
              &quot;racing ahead&quot; of the others.  In a one-way
              coupled system, this may mean the introduction of
              artificial <VAR>synchronization files</VAR> which exist
              solely to provide these feedbacks; and 
              <P>
        <LI>  Computing the <CODE>IOAPI_KEEP_NSTEPS</CODE> for each
              process.  This is the number of time steps that must be
              kept in PVM-mailbox memory for that process to function
              within the coupled-model system.  In a typical example,
              the MM5 output time steps might be 10 minutes, the SMOKE
              output time steps are 1 hour, and the AQM output time
              steps might be 20 minutes.  In this case, the AQM
              simulation may need to fall as much as an 80 minutes
              behind the MM5 simulation, because of lags waiting for
              interpolatable emissions, etc.  This represents
              <CODE>
              IOAPI_KEEP_NSTEPS&nbsp;=&nbsp;9&nbsp;=&nbsp;1&nbsp;+&nbsp;(80&nbsp;minutes)/(10&nbsp;minutes) 
              </CODE> time steps for MM5, 
              <CODE>IOAPI_KEEP_NSTEPS&nbsp;=&nbsp;3</CODE> for SMOKE
              (with its 1-hour time steps), and 
              <CODE>IOAPI_KEEP_NSTEPS&nbsp;=&nbsp;5&nbsp;=&nbsp;1&nbsp;+&nbsp;(80&nbsp;minutes)/(20&nbsp;minutes)
              </CODE> for the AQM.
    </UL>
    Using coupling mode to construct complex modeling systems has several 
    advantages from the model-engineering point of view:
    <UL>  
        <LI>  <STRONG>The same programs work unchanged both in standalone
              mode (reading input from files and writing output to
              files) and in coupled-model mode (reading and writing
              selected inputs or outputs to/from PVM mailboxes).
              </STRONG>
              <P>
         <LI>  Since data is tagged by variable-name, simulation date, 
              and time, the system is not subject to data scrambling
              because of implicit programming assumptions about the
              data ordering in the way that stream-like communications
              channels are.
              <P>
        <LI>  Readers and writers do not need to know about each other
              in detail.  In particular, any reader only needs to know 
              that <EM>some</EM> writer will put the variables it needs
              into the mailbox.  Writers don't care whether readers
              even exist or not.  It is easy to change system
              configuration by just adding additional processes or by
              deleting processes and replacing them by appropriate
              disk-based files containing  the data that would have
              been produced.  In MCNC's Real-Time Ozone Forecast
              System, for example, the set of programs that runs to
              compute each day's ozone forecast varies from day to day,
              on the basis of such things as whether particular data 
              ingest feeds have succeeded or failed over the past two
              days.
              <P>
        <LI>  One writer can supply multiple readers without special
              programming (and without needing to know who they are).
              For example, in a coupled system with the MM5/MCIP
              meteorology model, the SMOKE emissions model, and the 
              MAQSIP air quality model, MM5 produces 5 time-stepped
              output &quot;virtual files&quot;, some variables of two
              of which are read by SMOKE and all of which are read by
              MAQSIP; and SMOKE produces one output &quot;virtual
              files&quot; read by MAQSIP.  SMOKE is itself a system of
              five programs coupled together by virtual files and fed
              by a number of additional disk-files produced  off-line. 
              MAQSIP produces a &quot;synchronization file&quot; read
              by MM5/MCIP and used to keep MM5/MCIP from running ahead 
              and exhausting all memory available for mailbox-buffer
              space.
              <P>
    </UL>

    <P>
    <STRONG>Back to <EM><A HREF="#contents">Contents</A></EM></STRONG>
    <P>

<HR> <!----------------------------------------------------------------->

<A HREF = "MCPL.html" >
Previous:  <STRONG>MCPL I/O&nbsp;API output module for MM5</STRONG>
</A><P> 

<A HREF = "AIRS2M3.html" >
Next:  <STRONG>AIRS2M3 Program</STRONG>
</A><P> 

<A HREF = "AA.html#tools"> 
Up: <STRONG>I/O&nbsp;API Related Programs</STRONG> 
</A><P>

<A HREF = "AA.html"> 
To: <STRONG>Models-3/EDSS I/O&nbsp;API:   The Help Pages</STRONG> 
</A><P>

<HR> <!----------------------------------------------------------------->

Send comments to 
<A HREF = "mailto:carlie@jyarborough.com "> <ADDRESS> 
          Carlie J. Coats, Jr. <br> 
          carlie@jyarborough.com </ADDRESS> </A><P> 

<!--#include virtual="/INCLUDES/footer.html" -->
</BODY>      <!--end body  -->
</HTML>      <!--end html  -->

