
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML>
<HEAD>
<!-- "$Id: REQUIREMENTS.html 73 2017-12-13 14:57:51Z coats $" -->
<META NAME="MSSmartTagsPreventParsing" CONTENT="TRUE">
<TITLE> I/O&nbsp;API REQUIREMENTS</TITLE>
</HEAD>

<BODY BGCOLOR="#FFFFFF"
      TOPMARGIN="15"
      MARGINHEIGHT="15"
      LEFTMARGIN="15"
      MARGINWIDTH="15">

<!--#include virtual="/INCLUDES/header.html" -->

<H1>
        EDSS and Models-3  I/O:  <BR>
        REQUIREMENTS,  DESIGN,  AND  IMPLEMENTATION
</H1>

    <BLOCKQUOTE>
    Carlie J. Coats, Jr., Ph.D. <BR>
    Environmental Programs, <BR>
    MCNC North Carolina Supercomputing Center <BR>
    Copyright 1992-2001 MCNC
    </BLOCKQUOTE>

<H2> INTRODUCTION </H2>

EDSS is a system designed to support decision making and related
activities (such as modeling) for both regulation and research
concerning environmental issues.  That means  that EDSS is more than
just a model:  more, even, than a family  of models together with a
graphics and analysis package attached  to them.  Models-3 is the EPA's
National Environmental Research Laboratory's interoperable air quality
counterpart, which shares  many models, parts, etc.,with EDSS.  Within
both of these, however, lies a common thread:  tools which can create
and/or get to the  data and distill it in ways useful to decision
making, and do so  in a timely fashion.  Decisions need to be based
upon  <STRONG>facts</STRONG> -- upon environmental data which may be
the result of observations, or the result of modeling (whether  air
quality modeling or economic modeling or what.)  With the  initial
emphasis on air quality issues, EDSS will initially contain  at least a
family of air quality models, together with meteorology  and emissions
models to support them, impact models (economic,  ecosystem, etc.) to
assess their effects, and analysis tools to  examine their results.
Undistilled environmental data is so  voluminous, however, that just
&quot;looking at&quot; the numbers  (or any substantial subset thereof)
is not a profitable enterprise  for the decision maker.  As Hamming so
aptly put it,  &quot;The purpose of computing is insight, not
numbers.&quot;
<P>

The voluminous data associated with environmental issues leads us
to two different but related subjects which need to be handled by
different levels of the EDSS system:
<UL>
    <LI>  data management; and
    <LI>  data access.
</UL>

Data management is concerned with operations which affect data
sets as a whole:  indexing, archiving, and migration, while data
access is concerned with the ways that tools access data extracted
from data sets for use within themselves, whether for the purpose
of modeling or for the purpose of analyzing it in order to attain
insight.  This paper is concerned with the second issue, the issue
of data access; data management in a way which supports the needs
of EDSS is the issue of a separate paper.  Data access is concerned
with the ways that programs access data from data sets.  Three
issues come up in this regard:
<UL>
    <LI>  How do programs refer to data sets?

    <LI>  What operations need to be performed, and therefore what
          subroutines  perform these operations?

    <LI>  What data structures (and other common assumptions) are
          needed for  the interfaces to these subroutines?
</UL>

Specifying these three items cleanly and in a modular, re-usable
fashion leads to a data access interface for  programs, what the
current computer jargon will call an input/output  applications
programming interface, or I/O&nbsp;API.  This document is concerned
with describing the requirements, design, and implementation of the
Models-3/EDSS I/O&nbsp;API.  To some extent, this description will use
the language of object-oriented programming, although this treatment
is not thoroughgoing in that regard.  (Some would say that we are
taking an object-based rather than an object-oriented view.)  In O-O
terms, this combination of operations, or methods, to be performed on
data and structures for storing it are said to form a class.  From
the point of view of a user of a class, what is important is what the
operations are, rather than how they do it -- a concept called
encapsulation -- that emphasizes the distinction between externally
visible or public interfaces and internal or private implementations.
The idea of using a generalized class as a foundation for more
specific subclasses specialized for a particular purpose is called
<EM>inheritance</EM>.

It should be realized that the present implementations may not scale
to the long term, since it is unclear at present what the shape of
computing will be when it is dominated by massively parallel systems
with many thousands of processors.  The one thing which seems clear
at present is that parallel systems researchers are still groping
for the &quot;right&quot; way to exploit such parallelism.  With sufficient
forethought, however, the requirements analysis and interface design
may survive even if the implementation does not.


<H2> I.   REQUIREMENTS </H2>

    <H3> A:  Objective </H3>

The objective of the Models-3/EDSS I/O&nbsp;API is to provide a single
generalized I/O structure class (with subclasses) with the data
structures and data access operations to fulfill the needs of Models-3
and EDSS.  Initially, this class needs to serve the needs of meteorology,
emissions, and air quality models, preprocessors, postprocessors, analysis
and visualization programs, and such other computational tools are used
within Models-3 and EDSS, for both regulatory and research applications.


    <H3> B:  Systems Requirements </H3>

The I/O&nbsp;API must be callable from at least
<A HREF = "FORTRAN.html"> FORTRAN </A> and <A HREF = "C.html"> C </A>.
It must be stable over time and should be upward compatible with successive
versions of EDSS models and framework, and capable of fulfilling
Models-3 and EDSS' needs for the foreseeable future.  It must be
compatible with at least the following platforms, operating systems,
and other software systems:
<UL>
	<LI>  POSIX
	<LI>  Cray / UNICOS
	<LI>  UNIX
	<LI>  TCP/IP/FTP
	<LI>  NFS and AFS
</UL>

The data must be portable across machines; ideally, they should
be transparently accessible across a heterogeneous distributed
network using remote-mounted file systems such as NFS and AFS.
Files must contain sufficient self-description that they can
&quot;stand alone.&quot; In particular, reading they should
not depend upon the availability of additional external
&quot;grid description &quot; or &quot;file dictionary&quot; software.
<P>

The I/O&nbsp;API must be both friendly for research modelers and also
have adequate integrity to serve for regulatory applications.  In
particular, it must maintain &quot;chain of custody&quot; adequate to
stand up in court for the data it manipulates.  See the
<A HREF="STANDARDS.html">Models-3 Coding Standards Document</A> for
some of the relevant coding standards, in order to  achieve this level
of integrity.
<P>


    <H3> C:  Data Types Supported </H3>

For purposes of further analysis, we assume that data is organized
into logical files which are assumed to be <A HREF = "VBLE.html">
multiple-variable data sets </A> having a common origin and common
<A HREF = "DATATYPES.html#struct"> data structure type </A> (common
time-stepping assumptions, grid dimensions, etc.).  On the other hand,
emissions data shows that variables of different
<A HREF = "DATATYPES.html#basic"> basic data types </A>-- INTEGER,
REAL, and DOUBLE PRECISION -- may be required in the same file.
<P>


Preliminary investigation suggested that at least 60 variables per file
may be necessary (later experience has expanded this, so that the
current limit is 120 for versions through 3.0, and 2048 for versions 3.1
and later), and that it is useful for files to be able to hold data for
periods of at least a year in duration.  Whether the  logical files are
implemented as physical files or as tables in  a scientific database or
by some other means is an implementation  issue which may in fact change
over time; what is important for  analysis here is the nature of the
interface to these logical  files.  A systems analysis of meteorology,
emissions, and air  quality modeling finds a number of particular types
of data  which must be supported.  These particular types of data can
be fitted into several generalized data types which need to  be
supported by the I/O&nbsp;API, in keeping with the object-based
methodology we are employing.  These particular data types are:
<UL>
    <LI>  terrain-height data
    <LI>  land-use data
    <LI>  demographic data
    <LI>  meteorology observations:
    <UL>
    	<LI>  surface
    	<LI>  upper-air profiles
    	<LI>  satellite (this is a future item not currently in use)
        <LI>  radar (also a future item)
    </UL>
    <LI>  air quality monitor observations
    <LI>  deposition monitor observations
    <LI>  emissions input data:
    <UL>
    	<LI>  point-source
    	<LI>  area-source
    	<LI>  mobile-source
    	<LI>  natural-source (biogenic, lightning strike)
    </UL>
    <LI>  processed (modeled) meteorology data
    <LI>  processed (modeled) emissions data
    <LI>  initial condition data
    <LI>  boundary condition data
    <LI>  geospatial transform (sparse) matrices for emissions and other modeling
    <LI>  air quality concentration and deposition data
    <LI>  intermediate-stage modeling data
    <UL>
        <LI>  diagnostic
        <LI>  other research
    </UL>
    <LI>  <EM>Added Nov.&nbsp;2001: </EM> Additional types of data that
    it may be useful to support are the following (either of which may
    be time-independent or time-dependent):
    <UL>
        <LI>  variables defined on geospatial coverages
        <LI>  variables defined on finite element (unstructured) grids
    </UL>
    <EM> Note that the usual GIS formats and access methods do NOT
    provide efficient access to <STRONG>time-stepped</STRONG> variables
    defined on geospatial coverages.</EM>
</UL>

During the process of both input (of emissions) and analysis, we
are frequently concerned with aggregated data, data which has been
combined using such operations as averages, maxima, and minima
applied to particular subsets of the data.  The I/O&nbsp;API must support
the results of such aggregations.  Two important types of aggregation
operations are:
<UL>
    <LI>  temporally aggregated data (daily-maximum ozone concentrations
    or annual-total sulfate depositions, for example)

    <LI>  geographically aggregated data (such as state or county totals)
</UL>

Analysis of the types of data arising and the operations applied to
them yields that the the system must support the following:
<UL>
    <LI>  Read and write operations must do the status checking necessary
    to ensure whether they are successful, and maintain audit trails of
    all the relevant operations as they do so.  In particular, failing
    and/or incomplete read and write operations must be flagged as
    unsuccessful, and the nature of this failure logged.

    <LI>  both <A HREF = "DATATYPES.html#timestuff"> time-independent
    data and time-stepped </A> data, with
    <A HREF = "DATETIME.html"> time step </A> granularity ranging
    from very small (~ 1 second) to
    very large (~ 1 year), and should correctly support dates for
    the year 2000 and beyond.  Climate modeling requires
    support for dates 1970 and before.  Source-attribution
    modeling may require writing data in other than chronological
    order.

    <LI>  <A HREF = "VBLE.html"> multiple layers of multiple
    (at least 60 for full-chemistry aerosol modeling) variables</A>
    per data set.

    <LI>  at least the following general data structure types:
    <UL>

        <LI>  <A HREF = "DATATYPES.html#grdded"> gridded </A>
              (2-D and 3-D) data

        <LI> <A HREF = "DATATYPES.html#bndary"> boundary-condition </A> data

        <LI>  <A HREF = "DATATYPES.html#iddata"> ID-referenced </A>
              data (this is a generalization which manages  both
              geographically-scattered data such as most observational
              data, and geographically aggregated data such as state or
              county totals)

        <LI> <A HREF = "DATATYPES.html#profil"> vertical-profile </A>
              rawinsonde meteorology data (different enough to be its
              own special case)

        <LI>  <A HREF = "DATATYPES.html#grnest"> nested-grid and
              multiple-grid </A> data

        <LI> <A HREF = "DATATYPES.html#smatrx"> sparse-matrix </A> data

        <LI>  <A HREF = "DATATYPES.html#custom"> data with
              user-defined structure </A> (for extensibility in case
              the analysis missed something; the system treats the
              data as a BLOB of one of the
              <A HREF = "DATATYPES.html#basic">basic data types</A>
              (REAL, DOUBLE, or INTEGER); the user imposes any additional
              structure he desires).
    </UL>
    <LI>  <EM>Added Nov.&nbsp;2001: </EM> An additional datatype that
          we have prototyped, and that may be useful is that of
          <STRONG><A HREF = "gefile.html">geospatial-element cell
          complexes (GECC)</A></STRONG>, which is a type efficiently
          supporting both (time-stepped and time-independent)
          geospatial coverages and finite element data, and modeled
          after the data structures used in the (pure) mathematical
          field of geometric topology.
</UL>

All files must contain all the information necessary to access the data
contained in them.  This is important both for analysis, where  it
permits unified tools supporting a variety of files, and for  sharing
data with others:  the only data needing to be transported to a
colleague's system is the file itself, and not a collection of
auxiliary files as with some current models.
<A HREF = "INCLUDE.html#fdesc">At least the following sorts of descriptive
information are required: </A>
<UL>
    <LI>  file description (text about this type of file)

    <LI>  file type (gridded, grid-boundary, ID-referenced, profile,
          gridnest, sparse matrix, or custom)

    <LI>  time step (or 0 for time-independent)

    <LI>  starting date and time (relevant if time step nonzero)

    <LI>  number of variables, their names, units designations, and descriptions

    <LI>  update description (date and time of update; name of updating
    program; text concerning the computational model run which supplied
    the data in this file)

    <LI>  coordinate system type and specifications (e.g.,
          &quot;Lambert, with  defining angles  30N, 60N, 90W, and
          center at  40N, 90W&quot;).

    <LI>  data structure dimensionality (depends upon file type) --
          e.g., for gridded:
    <UL>
    	<LI>  number of columns
	<LI>  number of rows
	<LI>  number of levels
    </UL>

    <LI>  <A HREF = "GRIDS.html#horiz">horizontal grid geometry</A>,
          if relevant:
    <UL>
    	<LI>  location (X,Y) of the grid origin (SW corner)
	<LI>  cell-size (DX, DY)
    </UL>

    <LI>  Coordinate system and horizontal grid geometry specifications
          must be sufficiently precise  to support (the ill-conditioned
          arithmetic in) geospatial transforms for very-high-resolution
          (e.g., 10-meter) modeling.  <STRONG>In particular,
          <CODE>REAL*4</CODE> representation is not adequate.</STRONG>

    <LI>  Coordinate system and horizontal grid geometry specifications
          must be coded so as to support  questions like &quot;Is this grid a
          properly implemented nest into that one?&quot;

    <LI>  <A HREF = "GRIDS.html#vert">vertical grid geometry</A>,
          if relevant (if number of levels is greater than 1):
    <UL>
	<LI>  type of vertical coordinate system:
              <UL>
                  <LI>  hydrostatic sigma-P;
                  <LI>  nonhydrostatic sigma-P
                  <LI>  sigma-Z
                  <LI>  pressure
                  <LI>  height above sea level
                  <LI>  height above ground
                  <LI>  other
              </UL>
	<LI>  array of layer surfaces;
	<LI>  (for sigma-coordinates only:) the model-top
    </UL>
</UL>

    <H3> D:  Functional Requirements </H3>

The I/O&nbsp;API must support the access needs of both the environmental
models used to simulate situations of interest to decision makers and
also the analysis and visualization tools used to distill insight
from the model inputs and outputs.  Of particular concern is the
fact that Models-3 and EDSS will contain families of air quality models
with interchangeable-part science modules which implement the simulation
of the various relevant physical processes -- horizontal or vertical
advection, convective mixing, deposition, chemistry, etc., at a
variety of scales.  Supporting both the model structure and the
analysis tools suggests that the view of the data presented to
the programmer by the I/O&nbsp;API should be selective random access
in terms compatible with model usage (i.e., access by file,
variable, layer, date and time, with possible further selection
by grid location).  The I/O&nbsp;API should automate routine activities
such as the logging of I/O transactions to the extent feasible.
Examination of the initial air quality model prototypes has added
a pair of additional features:  time-interpolation to a particular
date and time should be added as an additional operation; and that
it should verify consistency between the file structure as requested
by the caller and the file structure as recorded in the file itself,
e.g., by an additional buffer-size argument.  The desired operations
are the following:
<UL>
    <LI>  <A HREF = "INIT3.html"> start up </A> the system;

    <LI>  <A HREF = "OPEN3.html"> create a new file </A>,
          according to a caller-supplied specification;

    <LI>  <A HREF = "OPEN3.html"> open an existing file </A>,
          either for input-only or for input/output;

    <LI>  <A HREF = "DESC3.html"> get the description </A>
          of an (existing) file;

    <LI>  read data from a file, with at least the following variants:
    <UL>
    	<LI>  <A HREF = "READ3.html"> read the data </A> for a
              specified date and time, variable, and layer (where
              &quot;all&quot; is a valid layer or variable specifier);

	<LI>  <A HREF = "XTRACT3.html"> read a subrectangle within
              the grid </A> of gridded data for a specified variable,
              range of dates and times, and subrectangle within the grid;

	<LI>  <A HREF = "INTERP3.html"> time-interpolate </A> gridded,
              boundary, or custom data for a  specified variable to a
              specified date and time (time interpolation of
              ID-referenced data might not be well-defined if,
              for example, differing sets of sites occur at adjacent
              time steps);
    </UL>

    <LI>  <A HREF = "WRITE3.html"> write </A> data for a specified
              variable, date and time to a file; and

    <LI>  <A HREF = "SHUT3.html"> shut </A> the system down,
          flushing all data to disk.
</UL>

    <H3> E:  Performance Requirements </H3>

In order to support both analysis and modeling, the system must support
operations on multiple (at least 20) simultaneously open files.  Model
nesting and model intercomparison imply that the system must support
simultaneous access to files for different domains (something not
possible with the current (1990's)  generation of some models).  Access
should  be by  meaningful name or meaningful value rather than by
arbitrary  index values (so that the caller asks for &quot;O3&quot; by
name for example,  rather than needing to know whether ozone is variable
# 17, and  requesting that).  As used by calling programs, file names
themselves  should be &quot;logical names&quot; in the sense that they
are properties  of the program using them, do not depend upon particular
physical file  names in the file system, and permit simultaneous and
independent execution  of different instances of the same program on the
same machine without  interference with each other (so that different
runs of the same air  quality model might be executing simultaneously on
the same machine,  for example).  Using only the globally-visible
namespace provided by  the file system makes this impossible -- or
difficult, at best -- in  many instances.


<H2> II.  DESIGN </H2>

The design of the Models-3/EDSS I/O&nbsp;API is given here in terms of its
externally visible properties, i.e., in terms of the conventions used,
the public INCLUDE-file interfaces, and the function-call interfaces for
the public routines in the I/O&nbsp;API.  This section documents these externally
visible properties from the <A HREF = "FORTRAN.html">FORTRAN</A>
programmers point of view, rather than from that of the
<A HREF = "C.html">C programmer, as documented in a separate section</A>.


    <H3> A:  Conventions </H3>

There are a number of data structuring and manipulation conventions
used consistently throughout the Models-3 and EDSS systems, and which
affect the I/O&nbsp;API.  Among these are the representation of object-names,
grids, dates, times, and time-deltas.  object names are (blank-padded)
FORTRAN CHARACTER strings of length at most 16.  Case is significant.
<P>

<A HREF = "GRIDS.html#horiz"> <STRONG>Horizontal coordinate
systems</STRONG> </A> are named entities, with map projections
taken from a short list of types:  Lat-Lon, Lambert conformal,
Mercator, and Stereographic.  Because of the ill-conditioned nature
of arithmetic relating to coordinate transformations, descriptive
parameters which completely specify the coordinate systems are kept
in 8-byte REALs.  For all these except Lat-Lon (for which the
parameters are ignored), specification of a map projection
requires three parameters to determine the map projection,
and two additional parameters to specify the coordinate-system
origin relative to that projection.
<P>

<A HREF = "GRIDS.html#horiz"> <STRONG>horizontal grids</STRONG> </A>
are named entities, for purposes of unambiguous identification.
For many models, it suffices to deal with regular grids, which are
completely characterized by the specification of a horizontal coordinate
system and four additional parameters which specify the grid origin
(lower-left corner) and the cell-size.  Irregular grids are specified
by grid-geometry files, which are gridded files specifying cell location
and extent on a cell-by-cell basis.
<P>

<A HREF = "GRIDS.html#vert"> <STRONG>vertical grids</STRONG> </A>
are presumed to be irregularly-spaced and are characterized by the
following;
<UL>
    <LI>  vertical coordinate type, from a short list:
    <UL>
        <LI>  hydrostatic sigma-P
        <LI>  nonhydrostatic sigma-P
        <LI>  sigma-Z
        <LI>  Z (m above ground)
        <LI>  H (m above sea level)
        <LI>  eta
        <LI>  specified by a geometry file
        <LI>  other
    </UL>

    <LI>  value of model-top (sigma-coordinates only)
    <LI>  number <CODE>NLEVS</CODE> of levels
    <LI>  array <CODE>VLEVEL( 0:NLEVS )</CODE> of values for the levels
</UL>

<A HREF = "DATETIME.html"> <STRONG>dates and times</STRONG> </A>
are stored as integers coding the Julian date and (24-hour) time
using the formulas
<PRE>
    JDATE = 1000 * YEAR  +  DAY
    JTIME = 100 * (100 * HOUR  +  MINUTE)  +  SECOND
          = 10000 * HOUR  +  100 * MINUTE  +  SECOND
</PRE>

where the year is specified using all four digits, the day number is
between 1 and 365 or 366 (depending upon leap year), hour is between
0 and 23, and minutes and seconds are between 0 and 59.  For example,
the date Feb. 2, 1993 is coded as the integer 1993033, and the time
3:46:53 PM as 154653.  When finer-grained resolution is required,
this two-integer representation is supplemented by a third component
which is a REAL between 0.0 and 1.0 representing fractions of a second.
This representation satisfies the granularity requirement of one-second
resolution, gives exact and machine-independent calculation of record
numbers within datasets, etc., correctly handles dates and times both
before 1970 and after 2000, and is easy for modelers to interpret and
manipulate within, e.g., a debugger.  time-deltas are stored using the
same conventions as times, except that they may have arbitrarily large
hours-fields, and may be either positive or negative (in the latter
case, all three fields are negative or zero:  -333 means a time step
backwards by three minutes and thirty-three seconds).  A variety of
utility routines are available for manipulating dates, times, and
time deltas, and which handle arbitrary time deltas correctly.
<P>

We recommend that the convention be adopted that all times are given
in GMT; however, this policy is by no means required by the system.


    <H3> B:  Files -- Logical Names and Physical Names </H3>

Rather than forcing the programmer to deal with hard-coded file
names or hard-coded unit numbers, the I/O&nbsp;API introduces the
concept of <A HREF = "LOGICALS.html"> logical file names </A>.
The modeler can define his or her own
logical names, which then become properties of the program.
Then at run-time the EDSS process manager (or the user who
writes his own shell-scripts) uses the UNIX setenv command
(or the VMS ASSIGN command) to connect up the logical names
to the physical file name of any &quot;real&quot; file desired.
For programming purposes, the significant facts are that the
names should not contain blanks (except as padding at the end:
<CODE>'foo '</CODE> is OK; <CODE>'f  oo'</CODE> is not), and
when they are used in subroutine calls are FORTRAN character
strings at most 16 characters long.


    <H3> C:  Data Structures for Input and Output </H3>

Each logical file has header attributes describing itself, and a
sequence of time steps divided into logical data records accessed
by <A HREF = "VBLE.html"> variable and layer </A>.
<A HREF = "DATETIME.html"> Dates and times and time-steps </A> are
represented as indicated in the preceding section.  All layers of all
variables are assumed to have the same time-step and
<A HREF = "DATATYPES.html"> data type (gridded, boundary, etc.) </A>
structure.There are three categories of
<A HREF = "DATATYPES.html#timestruct"> time step structure </A>
presently in use:
<UL>
    <LI>  <STRONG>time-independent</STRONG> files have time step = 0;
    the date and time  arguments to access functions are ignored when
    these access functions are applied to time-independent files;

    <LI>  <STRONG>time-stepped</STRONG> files have time step &gt; 0
    with the time step indicated;

    <LI>  <STRONG>restart</STRONG> or circular-buffer files, which have
    time step &lt; 0 with  actual time step the absolute value of the
    time step indicated, store exactly two active time steps of data
    (the &quot;even step&quot; and the &quot;odd step&quot;) and may be
    used either for communications buffers or as restart-data files, at
    a considerable savings in space over a normally time-stepped file used
    for the same purpose.
</UL>

There are currently eight types of data structure currently supported,
although the system is designed to permit the addition of extra types
in an upward-compatible fashion.  The present grid-nest type was actually
implemented as a test of this extensibility.  Each type except dictionary
has additional layer structure and array dimensionality structure as well.
Indexes for these are subscripted according to FORTRAN conventions
(i.e., starting with 1).  Layers are counted from bottom to top vertically;
rows are counted from bottom (south) to top (north) and columns are
counted from left (west) to right (east() horizontally.  The data structure
types identified by
<A HREF = "INCLUDE.html#magic"> &quot;magic number&quot; parameters </A>
defined in
<A HREF = "INCLUDE.html#parms"> INCLUDE-file PARMS3.EXT </A>.
Together with the magic-number values, the types are:
<UL>
    <LI>  Type -1:  <A HREF = "DATATYPES.html#custom">
    <STRONG>custom</STRONG> </A>  User-defined REAL data with
    one logical record
    per variable, layer, and time step, with structure interpreted by
    the user.  Record size (in words) is stored as the number of columns.
    This type of file may be used to handle situations otherwise
    unanticipated by the present requirements analysis.

    <LI>  Type 0:  <A HREF = "DATATYPES.html#dctnry">
    <STRONG>dictionary</STRONG> </A>  The &quot;reusable&quot;
    portions of a file description,  with a named-record structure
    (mapping onto the variables referenced by READ3()) to index the file
    descriptions.  This type should be considered as a tentative prototype
    step in file type management rather than a complete and lasting
    solution.  The fields in such a description are:
    <UL>
        <LI>  file type ID (custom, dictionary, etc.
        <LI>  time step
        <LI>  number of variables
        <LI>  number of layers
        <LI>  number of rows or maximum number of ID-referenced data sites
        <LI>  number of columns or custom words per record or maximum number of
        <LI>  profile levels
        <LI>  boundary thickness in cells (used for boundary files only)
        <LI>  coordinate type ID (lat-lon, Lambert, Mercator, etc.)
        <LI>  coordinate specification parameters
        <LI>  grid name
        <LI>  grid specification parameters
        <LI>  file description
        <LI>  list of variable names
        <LI>  list of units designations for variables
        <LI>  list of variable descriptions
    </UL>

    <LI>  Type 1:  <A HREF = "DATATYPES.html#grdded">
    <STRONG>gridded</STRONG> </A>  (usually regularly) gridded
    data having one
    logical record per time step, variable, and layer, with memory
    layout as in the FORTRAN declaration
    <PRE>
        REAL ARRAY( NCOLS, NROWS )
    </PRE>

    <LI>  Type 2:  <A HREF = "DATATYPES.html#bndary">
    <STRONG>boundary</STRONG> </A>  boundary data has one
    logical record per time
    step, variable, and layer.  Its structure is defined in terms of
    a thickened grid perimeter proceeding counterclockwise from the
    SW (1,1) corner.  The array size for one layer of data is computed
    in terms of the dimensions of the corresponding gridded data grid
    and the additional thickness parameter <CODE>NTHIK</CODE> according
    to the following formula
    <PRE>
        2 | NTHIK| * (NCOLS + NROWS + 2*NTHIK)
    </PRE>
    where <CODE>NTHIK &gt; 0</CODE> indicates an external boundary and
    <CODE>NTHIK &lt; 0 </CODE>
    indicates an internal boundary.  It has component subarrays along
    each edge of the grid, each layer of which is structured as follows:
    <PRE>
	REAL SOUTH( NCOLS + NTHIK, NTHIK )
	REAL EAST ( NTHIK, NROWS + NTHIK )
	REAL NORTH( NCOLS + NTHIK, NTHIK )
	REAL WEST ( NTHIK, NROWS + NTHIK )
    </PRE>

    <LI>  Type 3:  <A HREF = "DATATYPES.html#iddata">
    <STRONG>iddata</STRONG> </A>  ID-referenced data has one
    logical record per
    time step.  Note that such data as county-aggregation files may
    be treated as a special case by the use of some such encoding of
    the site-ID as the FIPS codes.  Note also that location parameters
    must be explicitly treated as variables if they are stored in such
    a file.  The data records are structured as follows (where MAX is
    the file attribute maximum number of sites):
    <UL>
    	<LI>  number of actual sites
              <CODE>INTEGER NSITES</CODE>
	<LI>  array of site ID's
              <CODE>INTEGER ID( MAX )</CODE>
	<LI>  array of data
              <CODE>REAL DATA( MAX, NLAYS, NVARS )</CODE>
    </UL>

    <LI>  Type 4:  <A HREF = "DATATYPES.html#profil">
    <STRONG>profile</STRONG> </A>  For geographically scattered
    vertical profile arrays of rawinsonde data referenced by ID or by
    location.  Note that location
    is DOUBLE PRECISION and is treated as potentially time-dependent
    (to match the behavior of rawinsonde profiles, which the NWS moves
    around from time to time).  The data has one logical record per
    time step, structured as indicated below (where MXLVL is the maximum
    number of vertical levels):
    <UL>
    	<LI>  number of actual sites
              <CODE>INTEGER NSITES</CODE>
	<LI>  array of site ID's
              <CODE>INTEGER ID( MAX )</CODE>
	<LI>  array of site level counts
              <CODE>INTEGER NLVL( MAX )</CODE>
	<LI>  array of site X-locations
              <CODE>DOUBLE PRECISION X( MAX )</CODE>
	<LI>  array of site Y-locations
              <CODE>DOUBLE PRECISION Y( MAX )</CODE>
	<LI>  array of site Z-locations
              <CODE>DOUBLE PRECISION Z( MAX )</CODE>
	<LI>  array of data
              <CODE>REAL DATA( MXLVL, MAX, NLAYS, NVARS )</CODE>
    </UL>

    <LI>  Type 5:  <A HREF = "DATATYPES.html#grnest">
    <STRONG>grid-nest or multiple-grid </STRONG> </A> is a data type
    implemented  largely as a test
    of how extensible the system was in terms of new data types.  Its
    structure is somewhat similar to profile, except that each time
    step has a potentially varying number of regular grids, each of
    which has a time-dependent 2-D dimensionality, location, and cell
    size.  The description of the storage order (which is quite tedious)
    is omitted here for the sake of brevity.

    <LI>  Type 6:  <A HREF = "DATATYPES.html#smatrx">
    <STRONG>sparse matrix</STRONG> </A> uses so-called
    &quot;skyline-transpose representation&quot; to store
    sparse matrices for use by the new emissions model (and
    possibly other programs that need it.
    The data has one logical record per time step, as indicated
    below, where MXROW is the number of rows in the matrix and
    MXCOL is the maximum number of active columns per row.
    <UL>
        <LI>  number of active cols per row <CODE>INTEGER  NC( MXROW )</CODE>
        <LI>  subscripts for active cols <CODE>INTEGER  IC( MXCOL, MXROW )</CODE>
        <LI>  coefficients for active cols <CODE>REAL     CC( MXCOL, MXROW )</CODE>
    </UL>
</UL><P>


    <H3> D:  Public Include-file Structures </H3>

There are three public <A HREF = "INCLUDE.html"> INCLUDE </A> files
in the FORTRAN interface to the I/O&nbsp;API.  They are the following:
<UL>
    <LI>  <STRONG>PARMS3.EXT</STRONG> contains dimensioning parameters
    and the standard file-type,  coordinate-system-type,
    &quot;All Layers&quot;, etc., token values for the FORTRAN
    interface to the I/O&nbsp;API.

    <LI>  <STRONG>FDESC3.EXT</STRONG> contains FORTRAN data structures
    (COMMONs) for a Models-3/EDSS I/O&nbsp;API file description, and is used
    to give name syntax for passing file description data between routines
    OPEN3 and DESC3 and their callers.  Requires PARMS3.EXT for dimensioning.

    <LI>  <STRONG>IODECL3.EXT</STRONG> contains declarations and usage
    comments for the public routines in the FORTRAN I/O&nbsp;API.
</UL><P>


    <H3>
    E:  Public Call Interfaces and Specifications
    </H3>

Except for INIT3(), which is an <CODE>INTEGER</CODE> function, the routines
in the I/O&nbsp;API are LOGICAL functions which return .TRUE. exactly when they
succeed (and .FALSE. otherwise).  In the examples below, the names
(FNAME for logical file name, VNAME for variable name, PNAME for program
name, CNAME for calling-routine's name) are CHARACTER*(*) of length at
most 16, RDFLAG is INTEGER, ARRAY is the output buffer for data access
routines, dates and times follow Models-3/EDSS conventions described above,
and LOGDEV is the INTEGER FORTRAN unit number for the program's log file.
From the functional point of view there are four groups of routines.
<UL>
    <LI>  INIT3(), OPEN3(), and DESC3() are related to initialization,
    <LI>  READ3(), XTRACT3(), and INTERP3() are related to data
          retrieval,
    <LI>  WRITE3() is related to data storage, and
    <LI>  SHUT3() is related to system shutdown.
</UL>
Note that for <A HREF = "DATATYPES.html#timestuff"> time-independent
files </A>, the date and time arguments are ignored by the data access
routines.  Data sets are &quot;stateless&quot; in the sense that access
operations may be done in any (meaningful) order -- a given time step of
a variable may be read many times, time steps may be read or written in
reverse (or even random) order, etc.
<P>

Integer function <A HREF = "INIT3.html"> INIT3() </A> initializes the
entire state for the I/O&nbsp;API, and returns the unit number for the
<A HREF = "ENVIRONMENT.html"> log-file </A> (which will be attached
to the file whose logical name is 'LOGFILE' if one exists, and to
standard output otherwise).  INIT3() may (should) be called multiple
times by application routines and programs in order to get the log-file's
unit number.  A typical call to INIT3() might look like the following:
<PRE>
    LOGDEV = INIT3()
    IF ( LOGDEV .LT. 0 ) THEN
    ...(can't proceed; probably couldn't open the log
    ... file.  Stop the program.)
    END IF
</PRE><P>


Logical function <A HREF = "OPEN3.html"> OPEN3 </A> opens files
according to the requested status, and writes a file summary to the
<A HREF = "ENVIRONMENT.html"> program log </A>.  For those files opened
for writing, it sets the update info in the file header.  May be called
multiple times with multiple files; if called repeatedly for a file
already open, it returns .TRUE. unless the request is for READ/WRITE
and the file is already open for READONLY.  Legal values for STATUS
are given in PARMS3.EXT:  1 for READONLY, 2 for READ/WRITE/UPDATE
of existing files, 3 for READ/WRITE for new files, and 4 for READ/WRITE
of unknown (whether new or old) files.  A typical call looks like:
<PRE>
    IF( .NOT. OPEN3( FNAME, STATUS, PNAME ) ) THEN
    ...process the error:  OPEN3 failed.
    END IF
</PRE><P>


Logical function <A HREF = "DESC3.html"> DESC3 </A> puts all the
descriptive data for the specified file into the standard file
description data structures in
<A HREF = "INCLUDE.html#fdesc"> FDESC3.EXT </A>.
A typical call looks like:
<PRE>
        IF( .NOT. DESC3( FNAME ) ) THEN
        ...process the error:  DESC3 failed.
        END IF
</PRE><P>


Logical function <A HREF = "INTERP3.html"> INTERP3 </A> provides
encapsulated read-and-time-interpolate functionality for gridded and
boundary data to EDSS programs.  It reads enough data from the specified
file to interpolate all layers of the single specified variable to the
specified date and time, after checking that the specified record-size
is correct for that file.  Internally it uses its own data buffers to
optimize the read-operations.  Note that for time-independent data,
&quot;interpolate&quot; is taken to mean &quot;copy&quot; and the
date and time are irrelevant.  A typical call looks like:
<PRE>
    IF( .NOT. INTERP3( FNAME, VNAME, CNAME, DATE, TIME,
 &amp;		RECSIZE, ARRAY ) ) THEN
    ...process the error:  INTERP3 failed.
    END IF
</PRE><P>


Logical function <A HREF = "READ3.html"> READ3 </A> reads data from
the specified file for the specified date and time, variable, and layer.
If the file is a dictionary file, the variable name is used as the
dictionary-entry index.  Tokens ALLAYS3 and ALLVAR3 from PARMS3.EXT
may be used to read all layers or all variables for the time step,
respectively.  A typical call looks like:
<PRE>
    IF( .NOT. READ3( FNAME, VNAME, LAYER, DATE, TIME, ARRAY  ) ) THEN
    ...process the error:  READ3 failed.
    END IF
</PRE><P>


Logical function <A HREF = "XTRACT3.html"> XTRACT3 </A> reads data
from the specified gridded file for the specified date and time,
variable, and ranges of rows, columns, and layers.  The row, column,
layer range may be shrunk down as far as a single cell, or may be
expanded to include the entire 3-D grid (although it may be less
efficient reading the entire grid than is READ3).  Token ALLVAR3
from PARMS3.EXT may be used to read all variables for the time
step.  A typical call looks like:
<PRE>
    IF( .NOT. XTRACT3( FNAME, VNAME, LAY0, LAY0, ROW0, ROW1,
    &amp;		COL0, COL1, DATE, TIME, ARRAY ) ) THEN
    ...process the error:  XTRACT3 failed.
    END IF
</PRE><P>


Logical function <A HREF = "WRITE3.html"> WRITE3 </A> writes either
an individual variable (for GRIDDED, BOUNDARY, or CUSTOM files only),
or an entire time step (all variables, all layers) of data for the
specified date and time to the specified file.  To write an entire
time step, VNAME should be 'ALL',  A typical call looks like:
<PRE>
    IF( .NOT. WRITE3( FNAME, VNAME, DATE, TIME, ARRAY ) ) THEN
    ...process the error:  WRITE3 failed.
    END IF
</PRE><P>


Logical function <A HREF = "SHUT3.html"> SHUT3 </A> flushes all
open files to disk and then closes them.  (Failure probably indicates
some unrecoverable file-system error, but the user at least should be
notified when that happens.  A typical call looks like:
<PRE>
    IF( .NOT. SHUT3( ) ) THEN
    ... SHUT3 failed.
    END IF
</PRE><P>


<H2> III.  IMPLEMENTATION </H2>

The first two Models-3/EDSS I/O&nbsp;API implementations are built on top of
UCAR's <A HREF = "https://www.unidata.ucar.edu/software/netcdf/">
netCDF</A> library.
It is largely is a modeler-oriented wrapper around netCDF calls, and
constructs files with particular structure defined in terms of sets
of attributes as indicated above.  For the most part, the implementation
is written in FORTRAN, and uses a number of lower-level subroutines to
manage the details of its operation.  There is a matching set of C
routines, which are for the most part wrappers around the Fortran
routines.  The interface consists of 65 FORTRAN-77
routines, 5 FORTRAN INCLUDE-files, 26 C routines, and three C include
file, with about 14000 lines of code.  In three particular places
it was necessary to do multi-language programming for the Fortran bindings.
First, it was necessary to write wrappers callable from FORTRAN around the
<VAR>getenv()</VAR> and <VAR>time()</VAR> system calls in order
to evaluate logical names and to get the current wall-clock time.
In addition, because of the necessity to do dynamic memory allocation
for the buffers used by INTERP3 (which, it should be noted, requires
a more general notion of dynamic allocation than that available in
Fortran 90), it is implemented as one module written in two parts --
a FORTRAN part responsible for managing the file name and variable-name
interface, and a C part responsible for buffer management and interpolation.
<P>

We have also implemented C interfaces with semantics matching
the FORTRAN interface for use by graphics and analysis programs.
(Presently, some EDSS visualization programs use a C module which
directly calls the netCDF C API in order to read EDSS data sets --
a potential source of inconsistency as EDSS expands and develops further.)
<P>


<H2> IV.  LIMITATIONS </H2>

A major limitation of the present implementation is the limits
imposed by 32-bit addressing within most UNIX file systems.  Model
management and data indexing within Models-3 and EDSS would both be
far easier if it were possible to keep the outputs of entire episodes within
single files, rather than being forced to &quot;chunk&quot; the episodes
into shorter segments just to fit within the 2 GB limits of most file
systems (or the even more stringent necessity of fitting within
&quot;small&quot; (less than 1 GB) physical devices.  Consider that
the primary output file for a single ozone episode might have the
following dimensions:
<PRE>
    30 days, at
    24 (hourly) time steps per day, for
    60 variables, on a grid with
    100 columns
    100 rows
    25 layers, for a total data volume of
    43.2 GB, assuming single precision (4 bytes per number) storage.
</PRE>

<EM>NOTE</EM>:  For hydrological applications, the I/O&nbsp;API
has been used for much larger data sets than these (and appropriately
designed I/O&nbsp;API based analysis and visualization tools were
routinely used with):
<PRE>
    33 years, at
     4 (6-hourly) time steps per day, for
     8 variables, on a grid with
  2760 columns
  3320 rows
     1 layer, for a total data volume of
     3.53 TB
</PRE>

Another major limitation has to do with massively parallel
supercomputers, for which the &quot;correct&quot; I/O semantics
is a matter of research as of this writing, rather than a matter
of settled practice.
<VAR>
NOTE added OCT. 24, 1997:
<A HREF = "NEWSTUFF.html#datap">Various prototypes</A> for domain
decomposition data parallel models have been implemented and we are
evaluating them as part of the MCNC Environmental Program's
<A HREF = "http://www.baronams.com:/projects/ppar/index.html">Practical
Parallel Project</A>
</VAR>
<P>


<H2> V.  FUTURE EXTENSIONS </H2>

    <H3> A:  Data Types </H3>

One obvious kind of future extension is in the set of data types
supported.  There are several candidates, none of which is yet
sufficiently developed that we can specify them in detail.  A first
candidate is new data types designed to better structure emissions
data in connection with EDSS improvements to emissions modeling.
A second candidate is a data type designed to deal with finite-element
or finite-volume data on unstructured meshes.  A third candidate is
exchange-flux matrices to support air quality models incorporating
the results of generalized-chemistry research being performed by
Prof. Harvey Jeffries of the University of North Carolina at Chapel Hill.
<P>
<EM></EM>


    <H3> B:  Communication for Parallel Computing </H3>

<VAR>
NOTE added OCT. 24, 1997:
<A HREF = "NEWSTUFF.html#coupl">The following has been
implemented</A> and we are evaluating it as part of the
<A HREF = "//projects/ppar/index.html">Practical
Parallel Project</A>
</VAR>
<P>

Another possible kind of future extension is in structuring
communication and coordination for parallel programs.  If the
I/O&nbsp;API had two modes -- a communications mode in addition to
the existing file storage mode -- it could be use to structure
well-engineered coupled models and parallel models in the
following fashion:  In the communications mode, the read
operations must be selective by simulation-time (as they
are now), and must block (i.e., suspend the execution of
their calling process) until the data for the time requested
becomes available.  One would then construct coupled or
parallel models by building an ordinary program for each
component, capable of execution as a stand-alone model when
the I/O&nbsp;API is used in file storage mode.  When the programs
are executed at the same time, the coupled models would use
the communications mode of the I/O&nbsp;API to exchange data.
The scheduling for coupled models is performed implicitly
by the operating system (using the blocking nature of the
read operations to determine the order of execution), without
the developer having to construct an explicit scheduler for
the processes being simulated.  This methodology for constructing
coupled models requires the right sort of underlying interprocess
communications tools upon which to build, and does incur the
corresponding communications overheads (which, one hopes, are
small in comparison to the computational overheads of the
component models themselves).  However, it does seem to offer
several advantages:
<UL>
    <LI>  It supports good software engineering principles (modularization
    and encapsulation), since each of the components must deal with
    only a single sort of simulation.

    <LI>  It makes for easier re-use of code, since each component is a
    functioning environmental model in its own right.

    <LI>  It leads to smaller and simpler software systems, since scheduling
    is supplied by the operating system (and its interaction with the
    I/O&nbsp;API), and the developer need not worry about interactions
    between the component simulations.

    <LI>  It provides for the decomposition of the modeling system into
    explicitly parallel components (which may possibly be distributed
    to different host machines, if the underlying communications layer
    permits it.)  Hence it provides one approach to the use of MIMD
    massively parallel machines.
</UL><P>

One important potential application is the construction of
nested atmospheric models, possibly several levels of nesting
deep.  In such a nest-model system, there is an explicit nest
interaction science-process module in all except the highest
resolution models, which is responsible at every model time
step for aggregating nest results over the model's grid, and
then broadcasting boundary conditions for all the models nested
within it.  The remainder of the science process modules (and
the remainder of the individual models themselves) are otherwise
unchanged.  The one requirement for synchronization is that the
high resolution models' time steps divide exactly into the time
step of the parent in which they are nested.  If this approach
is used, the same component models could be used for both one-way
and two-way nesting (they need not even know whether they are
operating one-way or two-way!).
<P>

Another family of applications is the coupling of different types of
environmental models -- perhaps meteorology, emissions, and air quality
at first -- possibly at high-resolutions time scales that are impractical
otherwise because the data volume would overwhelm all available disk
space if the data were stored there.  If, however, the meteorology
data volume is kept in temporary communication memory rather than
on disk, the problem is avoided.
<P>

Another application of a communication mode of the I/O&nbsp;API might be to
use it to achieve domain-decomposition parallelism for the distributed
execution of environmental models:  First, decompose the geographic
domain into subdomains.  On each of the subdomains, run a copy of the
environmental model, and a master modeling program whose task is to
assemble the results from the subdomains into a coherent whole on the
entire domain, and then to broadcast boundary conditions to each of
the subdomain models.  This may well be the paradigm by which we get
air quality models to efficiently use the resources of MPP machines
while at the same time writing well-engineered, maintainable systems.
<P>

<EM><STRONG>Added 1997: </STRONG>
This extension, the
<A HREF = "http://www.baronams.com:/projects/ppar/ioapi.html">coupling mode of the
I/O&nbsp;API</A>, has been developed under the aegis of the MCNC
Environmental Programs
<A HREF = "http://www.baronams.com:/projects/ppar/">Practical Parallel Computing Strategies
Project</A>, a project partially funded by US EPA.  It has proved
very useful for constructing coupled modeling systems, such as that
used for <A HREF = "http://www.baronams.com:/projects/SECMEP/">MCNC numerical air quality
forecasting</A> and  for <A HREF = "http://www.baronams.com:/projects/dashmm/">coupled
hydrological-meteorological modeling</A>.
</EM> <P>

<H3> Other Extensions </H3>

At some point, it might be worthwhile to implement C++ interfaces
with a full-blown class structure for files, variables, layers,
dates and times, etc., which fully supports the structure of the
data.  Since the requirements analysis and the design were object
based (with inheritance implemented in terms of call hierarchy and
&quot;cut, paste, andedit&quot; instead of the implementation language),
it should be possible to do so.  It would, however, be a nontrivial task :-).
<P>

<EM><STRONG>Added Nov.&nbsp;2001: </STRONG> MCNC Environmental Modeling
Center has prototyped a <STRONG>
<A HREF = "gefile.html">geospatial-element cell complex
(GECC)</A></STRONG> datatype that efficiently supports both
(time-stepped and time-independent) geospatial coverages and finite
element data, on cell complexes with either time-stepped and
time-independent node coordinates.
</EM>
<P>

<HR> <!------------------------------------------------------------>

<A HREF = "https://www.unidata.ucar.edu/software/netcdf/" >
Previous:  <STRONG>netCDF User's Guide</STRONG>
</A><P>

<A HREF = "NOTICES.html" >
Next:  <STRONG>Notices:  Copyright, Acknowledgements</STRONG>
</A><P>

<A HREF = "AA.html">
To: <STRONG>Models-3/EDSS I/O&nbsp;API:   The Help Pages</STRONG>
</A><P>

<!--#include virtual="/INCLUDES/footer.html" -->

</BODY>
</HTML>

